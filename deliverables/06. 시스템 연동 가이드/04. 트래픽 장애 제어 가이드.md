- [1. 개정이력](#1-%EA%B0%9C%EC%A0%95%EC%9D%B4%EB%A0%A5)
- [2. Audience](#2-audience)
- [3. Spring cloud hystrix](#3-spring-cloud-hystrix)
  - [3.1. 개요](#31-%EA%B0%9C%EC%9A%94)
  - [3.2. dependency 추가](#32-dependency-%EC%B6%94%EA%B0%80)
  - [3.3. java main() 클래스 설정](#33-java-main-%ED%81%B4%EB%9E%98%EC%8A%A4-%EC%84%A4%EC%A0%95)
  - [3.4. application.yaml 추가 (default 전역 설정)](#34-applicationyaml-%EC%B6%94%EA%B0%80-default-%EC%A0%84%EC%97%AD-%EC%84%A4%EC%A0%95)
  - [3.5. java method 설정](#35-java-method-%EC%84%A4%EC%A0%95)
  - [3.6. java class 설정](#36-java-class-%EC%84%A4%EC%A0%95)
- [4. Kubernetes Resources 및 liveness/readnessProbe](#4-kubernetes-resources-%EB%B0%8F-livenessreadnessprobe)
- [5. Kubernetes Autoscaling](#5-kubernetes-autoscaling)
  - [5.1. 개요](#51-%EA%B0%9C%EC%9A%94)
  - [5.2. 설정 방법 1](#52-%EC%84%A4%EC%A0%95-%EB%B0%A9%EB%B2%95-1)
  - [5.3. 설정 방법 2](#53-%EC%84%A4%EC%A0%95-%EB%B0%A9%EB%B2%95-2)
  - [5.4. 동작 예제](#54-%EB%8F%99%EC%9E%91-%EC%98%88%EC%A0%9C)
- [6. Istio](#6-istio)
  - [6.1. VirtualService](#61-virtualservice)
  - [6.2. DestinationRule](#62-destinationrule)


# 1. 개정이력

|    날짜    | 변경내용  | 작성자 | 비고 |
| :--------: | :-------: | :----: | :--: |
| 2019.03.08 | 최초 작성 | 전원호 |      |
|            |           |        |      |
|            |           |        |      |



# 2. Audience

- Spring cloud hystrix 라이브러리를 가지고 application layer에서 Circuit breaker를 구현하고자 하는 자.
- k8s Pod 리소스 관리를 통해 cpu 및 memory 할당량을 제어하고자 하는 자.
- k8s HPA(HorizontalPodAutoscaler) 오브젝트를 통해 cpu 사용률 기준값에 따른 Pods의 replicas 설정을 자동으로 설정하고자 하는 자.
- Istio 솔루션을 활용하여 application layer가 아닌 Infra layer에서 timeout 및 Circuit breaker 등 트래픽 라우팅 및 정책을 설정하고자 하는 자.



# 3. Spring cloud hystrix



## 3.1. 개요

![20190318110408](../06.%20%EC%8B%9C%EC%8A%A4%ED%85%9C%20%EC%97%B0%EB%8F%99%20%EA%B0%80%EC%9D%B4%EB%93%9C/assets/20190318110408.png)

컨테이너 클러스터 환경에서 어플리케이션은 Micro Services 간에 트래픽이 자주 발생한다. (기존 VM 장비의 WAS 인스턴스에서 여러가지 서비스가 monolithic하게 동작하였던 것과 비교하여 보자.) 위 그림을 예시로 supplier 서버에 장애가 생겨 항상 Timeout이 발생하는 경우, supplier 서버를 호출한 client 서버는 Timeout이 발생할 때까지 응답이 밀리게 된다. 응답이 밀리는 동안 요청이 계속 쌓여 결국 client 서버까지 요청이 과하게 밀려 장애가 발생할 수 있다. 이러한 상황에 circuit breaker를 두어 장애 전파를 막을 수 있다.

Micro Service Architecture에서 Micro Services간 트래픽이 많고 트래픽에 대한 관리가 중요하다. circuit breaker를 두어 Micro Services간 장애 전파 방지를 할 수 있다. 그러나  circuit breaker를 실제 적용함에 있어 쓰레드 개수 및 세마포어 사용여부, 캐싱 여부 등에 따라 성능 최적화 작업에 주의해야 한다.



## 3.2. dependency 추가

```xml
<properties>
    <spring-cloud.version>Greenwich.SR1</spring-cloud.version>
</properties>

<dependencies>
    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-starter-netflix-hystrix</artifactId>
    </dependency>
</dependencies>

<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.springframework.cloud</groupId>
            <artifactId>spring-cloud-dependencies</artifactId>
            <version>${spring-cloud.version}</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
```



## 3.3. java main() 클래스 설정

@EnableCircuitBreaker 클래스 annotation

```java
@SpringBootApplication
@EnableCircuitBreaker
public class Svc1Application {

	public static void main(String[] args) {
		SpringApplication.run(Svc1Application.class, args);
	}
	
	
	@Bean
	public RestTemplate restTemplate() {
		
		HttpComponentsClientHttpRequestFactory httpComponentsClientHttpRequestFactory = new HttpComponentsClientHttpRequestFactory();
		
		// default timeout setting
		httpComponentsClientHttpRequestFactory.setReadTimeout(1000*10);
		httpComponentsClientHttpRequestFactory.setConnectTimeout(1000*3);
		
		return new RestTemplate(httpComponentsClientHttpRequestFactory);
	}
}
```



## 3.4. application.yaml 추가 (default 전역 설정)

```yaml
hystrix:
  command:
    default:
      execution.isolation.thread.timeoutInMilliseconds: 5000
      metrics.rollingStats.timeInMilliseconds: 10000
      circuitBreaker.requestVolumeThreshold: 20
      circuitBreaker.errorThresholdPercentage: 50
      circuitBreaker.sleepWindowInMilliseconds: 5000
  threadpool:
    default:
      coreSize: 10
```

- execution.isolation.thread.timeoutInMilliseconds

Hystrix 가 적용된 메서드의 타임아웃을 지정한다. 이 타임아웃 내에 메서드가 완료되지못하면 서킷브레이커가 닫혀있다고 하더라도 fallback 메서드가 호출된다. 보통 외부 API 를 호출하게되면 RestTemplate 과 같은 http client에도 connect, read timeout 등을 지정하게하는데 hystrix timeout은 이를 포함하고 여유를 좀 더 두어 잡는다. 기본값은 1초(1000)

- metrics.rollingStats.timeInMilliseconds

서킷 브레이커가 열리기위한 조건을 체크할 시간이다. 아래에서 살펴볼 몇가지 조건들과 함께 조건을 정의하게되는데 "10초간 20건의 호출 중 50% 실패하면 서킷 브레이커 발동" 이라는 조건이 정의되어있다면 여기서 10초를 맡는다. 기본값은 10초(10000)

- circuitBreaker.requestVolumeThreshold

서킷 브레이커가 열리기 위한 최소 요청조건이다. 즉 이 값이 20으로 설정되어있다면 10초간 19개의 요청이 들어와서 19개가 전부 실패하더라도 서킷 브레이커는 열리지않는다. 기본값은 20

- circuitBreaker.errorThresholdPercentage

서킷 브레이커가 발동할 에러 퍼센트를 지정한다. 기본값은 50

- circuitBreaker.sleepWindowInMilliseconds

서킷 브레이커가 열렸을때 얼마나 지속될지를 설정한다. 기본값은 5초(5000). 회로가 열린 동안, 모든 요청에 대해서 fallback method을 바로 실행한다.  지정된 시간이 지난 후 하나의 요청을 원래 method로 실행(`HALF OPEN`). 이 요청이 실패한다면 `OPEN`으로 두고, 이 요청이 성공한다면 `CLOSED`로 상태를 변경한다.

- coreSize

circuitBreaker 동작 방식에는 Thread 방식과 Semaphore 방식이 있다. Thread 를 이용할 경우 ThreadPool 개수, core size를 지정하는 속성이다. Netflix에서는 특별한 케이스가 아닌 이상 Thread 방식을 권장한다.(디폴트 설정도 Thread 방식이다.) 기본값은 10



## 3.5. java method 설정

@HystrixCommand를 method 단위 annotation하고 동일한 파라미터와 리턴값을 값을 갖는 fallbackMethod 를 구현한다. circuit breaker가 OPEN되면 호출되는 method이다.

```java
@PostMapping(value = "/members", consumes= {MediaType.APPLICATION_FORM_URLENCODED_VALUE,MediaType.APPLICATION_JSON_UTF8_VALUE}, produces = MediaType.APPLICATION_JSON_UTF8_VALUE)
@HystrixCommand(fallbackMethod = "createMemberFallback",
					commandProperties = {@HystrixProperty(name = "execution.isolation.thread.timeoutInMilliseconds", value = "3000")
										,@HystrixProperty(name = "metrics.rollingStats.timeInMilliseconds", value = "10000")
										,@HystrixProperty(name = "circuitBreaker.requestVolumeThreshold", value = "20")
										,@HystrixProperty(name = "circuitBreaker.errorThresholdPercentage", value = "50")
										,@HystrixProperty(name = "circuitBreaker.sleepWindowInMilliseconds", value = "5000")
					},
					threadPoolProperties = {@HystrixProperty(name = "coreSize", value = "10")											
					}
	)
public ResponseEntity<Response> createMember(Request req) {

    logger.info("req => " + req.toString());

    req.setId(req.getId()+"_members");

    logger.info("# svc2 http://" + svc2Host + ":" + svc2Port + "/svc2/api/members/api2 POST 호출");

    // RestTemplate을 이용하여 svc2 서비스 Http Request 호출하기
    HttpHeaders headers = new HttpHeaders();
    headers.setContentType(MediaType.APPLICATION_JSON_UTF8);
    HttpEntity<Request> requestHttpEntity = new HttpEntity<Request>(req, headers);

    logger.info("# requestHttpEntity.getBody().toString(): " + requestHttpEntity.getBody().toString());
    logger.info("# properties - services.host.svc2: " + svc2Host);
    logger.info("# properties - services.port.svc2: " + svc2Port);

    ResponseEntity<Response> responseEntity = restTemplate.exchange("http://" + svc2Host + ":" + svc2Port + "/svc2/api/members/api2", HttpMethod.POST, requestHttpEntity, Response.class);
    logger.info("# responseEntity.getBody().toString(): " + responseEntity.getBody().toString());
    responseEntity.getBody().setResultMsg("Member(svc2) user created!!!!");

    //DB Insert
    logger.info("# createConnectionHistory");
    //sqlMapperDao.createConnectionHistory(req);

    return responseEntity;
}

public ResponseEntity<Response> createMemberFallback(Request req) { 

    logger.info("req => " + req.toString());

    req.setId(req.getId()+"_members");

    logger.info("# svc2 http://" + svc2Host + ":" + svc2Port + "/svc2/api/members/api2 POST 호출  Fallback");

    Response response = new Response();
    response.setId(req.getId());
    response.setName(req.getName());
    response.setSecurity_number(req.getSecurity_number());
    response.setResultCode("Fallback");
    response.setResultMsg("svc1=>svc2 Fallback");

    ResponseEntity<Response> responseEntity = new ResponseEntity<Response>(response, HttpStatus.OK);
    return responseEntity;
}
```

- commandKey의 default 값은 method 명이 된다.
- groupKey의 default 값은 class 명이 된다. groupKey는 commandKey의 그룹의 개념으로 단지 reporting, alerting, dashboards, or team/library ownership의 용도로 사용될 뿐이다.

- threadPoolKey의 default 값은 groupKey 값이 된다.
- commandKey, threadPoolKey에 대한 설정값이 정의되지 않았다면 default 키 설정된 값으로 동작한다.
- threadPoolKey 값은 독립된 Thread Pool 사용을 보장하며 동일 키나 정의되지 않아 default  키 사용의 경우 동일한 Thread Pool을 공유함에 주의하여야 한다.



## 3.6. java class 설정

클래스 레벨에 hystrix 설정도 가능하다.

```java
@RestController
@RequestMapping("/api/users")
@DefaultProperties(groupKey="",threadPoolKey="",commandProperties= {},threadPoolProperties= {})
public class UserApiController {
```



# 4. Kubernetes Resources 및 liveness/readnessProbe

Deployment object 내

```yaml
        livenessProbe:
          httpGet:
            path: /athn/actuator/health
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /athn/actuator/health
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10
        resources:
          requests:
            memory: "1250Mi"
          limits:
            cpu: "2000m"
            memory: "1250Mi"
```

- livenessProbe: 컨테이너의 상태가 비정상이라고 판단하면, 해당 Pod를 재시작

  - http, tcp, sh 방식이 존재하고 spring boot의 경우 actuator 기능을 활용하여 위와 같이 http 방식을 활용한다.
  - tcp 방식 참조

  ```yaml
           livenessProbe:
             tcpSocket:
               port: 80
             initialDelaySeconds: 30
             periodSeconds: 10
  ```

- readinessProbe: 컨테이너가 상태가 비정상이라고 판단하면, 해당 Pod를 사용할 수 없음으로 표시하고, 서비스등에서 제외

  - http, tcp, sh 방식이 존재하고 spring boot의 경우 actuator 기능을 활용하여 위와 같이 http 방식을 활용한다.
  - tcp 방식 참조

  ```yaml
           readinessProbe:
             tcpSocket:
               port: 80
             initialDelaySeconds: 30
             periodSeconds: 10
  ```

- restartPolicy: Always (Always, OnFailure, and Never. default는 Always)

- Pod 내 resources 절을 통하여 cpu 및 memory 크기에 대한 기동 시와 max 제한 값을 설정할 수 있다. 요청 시 memory는 1.25G, 제한은 cpu 2 cores와 memory 1.25G로 제한한다. 

```yaml
         resources:
           requests:
             memory: "1250Mi"
           limits:
             cpu: "2000m"
             memory: "1250Mi"
```



# 5. Kubernetes Autoscaling

Horizontal pod autoscaler는 rc, deployment, rs의 Pods 개수에 대한 자동 스케일링을 지원한다.



## 5.1. 개요

Horizontal Pod Autoscaler는 CPU 사용량(또는 베타 지원의 다른 애플리케이션 지원 메트릭)을 관찰하여 rc, deployment, rs와 같이 Pod 크기 조정이 가능한 오브젝트에 대하여 Pod 개수를 자동으로 스케일한다.

kube-controller-manager 설정(**--horizontal-pod-autoscaler-sync-period** 플래그)에 의해 15초 주기를 가지고 정의에 지정된 메트릭에 대해 리소스 사용률을 질의한다. 

CPU에 대한 오토스케일링 지원만 포함하는 안정된 버전은 `autoscaling/v1` API 버전에서 찾을 수 있다. 메모리 및 사용자 정의 메트릭에 대한 스케일링 지원을 포함하는 베타 버전은 `autoscaling/v2beta2`에서 확인할 수 있다. 



## 5.2. 설정 방법 1

```bash
$ kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
```

위와 같이  cpu 사용률 50%를 유지하기 위해  replica를 1~10까지 auto scaling하도록 설정한다.



## 5.3. 설정 방법 2

동일하게 yaml 파일로 관리할 수 있다.

```yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50
```



## 5.4. 동작 예제



```bash
$ kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
horizontalpodautoscaler.autoscaling/php-apache autoscaled
```

```shell
$ kubectl get hpa
NAME         REFERENCE                     TARGET    MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache/scale   0% / 50%  1         10        1          18s
```

이후 부하를 발생하면,

```bash
$ kubectl get hpa
NAME         REFERENCE                     TARGET      CURRENT   MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache/scale   305% / 50%  305%      1         10        1          3m
```

`CURRENT`은 디플로이먼트에 의해 제어되는 파드들의 평균을 나타낸다.

```shell
$ kubectl get deployment php-apache
NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
php-apache   7         7         7            7           19m
```



# 6. Istio

Service Mesh Architecture의 솔루션으로 Micro Service Architecture 구조에서 기존 보다 잘게 쪼개진 서비스 간 연동 호출 구조가 복잡해지고 이로 인한 Network 부하의 단점을 보완하기 위해 Pod 내 istio-proxy를 Side-car 패턴의 envoy를 위치시켜 동작하는 솔루션이다.



## 6.1. VirtualService

VirtualService는 트래픽 라우팅과 관련된 여러가지 설정이 가능하다. 다양한 라우팅 규칙 설정, timeout, fault injection(abort/delay), http redirect, http retry, http rewrite, weight 가중치 설정이 가능하다. ingressgateway pod에 해당 Virtualhost에 대한 라우팅 정책(트래픽 관리)을 등록하는 것이다.

상세한 사용 가이드는 https://istio.io/docs/reference/config/networking/v1alpha3/virtual-service에서 확인 가능하다.

`01. 클러스터 내 Service 외부 expose 가이드`에서 설명하듯이 Gateway object와 쌍을 이루어 ingressgateway pod에서 호출하는 서비스에 대한 설정이 가능하고 이후 Service object에 대한 독립적인  VirtualService 설정도 가능하다.



- timeout 설정 및 traffic shifting 설정 방법

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: onm
  labels:
    app: onm
    version: 1.0.0
    release: mmp
spec:
  hosts:
  - onm
  http:
  - route:
    - destination:
        host: onm
        port:
          number: 80
        subset: v1
      weight: 50
    - destination:
        host: onm
        port:
          number: 80
        subset: v2
      weight: 50
    timeout: 10s
```





## 6.2. DestinationRule

DestinationRule은 VirtualService에 의한 트래픽 라우팅 이후 동작하는 트래픽 정책에 대한 설정을 한다. 로드발런싱, 커넥션 풀, circuit breaker, sub Version=subset 설정이 가능하다.

subset은 Service object 수준에서 overriding 되어 selector 정보에 추가되는 원리이다.

```yaml
kind: DestinationRule
metadata:
  name: athn
  labels:
    app: athn
    version: 1.0.0
    release: mmp
spec:
  host: athn
  subsets:
  - name: v1
    labels:
      version: 1.0.0
  - name: v2
    labels:
      version: 1.1.0
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 50
        connectTimeout: 5s
      http:
        #http1MaxPendingRequests: 1 # default 1024
        #http2MaxRequests: 1 # default 1024
        #maxRequestsPerConnection: 1
        #maxRetries: 3  # default 3
    outlierDetection:
      consecutiveErrors: 20
      interval: 10s
      baseEjectionTime: 15s
      maxEjectionPercent: 100
    loadBalancer:
      consistentHash:
        #useSourceIp: true
        httpCookie:
          name: JSESSIONID
          ttl: 10800s
    tls:
      mode: ISTIO_MUTUAL
```

- subsets

subset v1, v2는 athn Service에 overriding 되어 version label을 가지고 v1, v2 sub version을 관리한다.

```yaml
  subsets:
  - name: v1
    labels:
      version: 1.0.0
  - name: v2
    labels:
      version: 1.1.0
```

- trafficPolicy

Spring cloud의 hystrix 기능과 동일한 기능을 제공한다. interval: 10s 동안 consecutiveErrors: 20 건의 5xx대 서버 에러가 발생하면 baseEjectionTime: 15s 동안 대상 서비스의 호출을 제외한다.

```yaml
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 50
        connectTimeout: 5s
      http:
        #http1MaxPendingRequests: 1 # default 1024
        #http2MaxRequests: 1 # default 1024
        #maxRequestsPerConnection: 1
        #maxRetries: 3  # default 3
    outlierDetection:
      consecutiveErrors: 20
      interval: 10s
      baseEjectionTime: 15s
      maxEjectionPercent: 100
```

- loadBalancer

기본적으로 round robin 방식으로 pod를 호출하지만 아래와 같은 설정으로 Cookie의 JSESSIONID 정보를 활용하여 hash모드로 Sticky session 로드발런싱 정책 설정이 가능하다.

```yaml
    loadBalancer:
      consistentHash:
        #useSourceIp: true
        httpCookie:
          name: JSESSIONID
          ttl: 10800s
```









